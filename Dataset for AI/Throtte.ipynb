{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize, Bounds, NonlinearConstraint\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RBFKernel, ScaleKernel\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstantMean\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:533\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    535\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gpr\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', RuntimeWarning)\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "from scipy.optimize import minimize, Bounds, NonlinearConstraint\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.optim import LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "exp_w = 2.6\n",
    "#C0varf = np.loadtxt('')\n",
    "FR = np.array([0.100, 0.075, 0.075])\n",
    "R_Frac = 1e-6\n",
    "ub = np.array([423, 423])\n",
    "lb = np.array([303, 303])\n",
    "bounds = Bounds((0, 0), (1, 1))\n",
    "dim = len(ub)\n",
    "kernel = gpr.kernels.Matern((1, 1), ((0.06, 5), (0.06, 5)), nu = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialspltvar = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " timespltvar= np.zeros(trialspltvar+1)\n",
    "timespltvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_vp = np.zeros(trialspltvar)\n",
    "time_vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCB_AF():\n",
    "    def __init__(self, model, dim, exp_w, descale, refmod = None, args = ()):\n",
    "        self.model = model\n",
    "        self.dim = dim\n",
    "        self.exp_w = exp_w\n",
    "        self.descale = descale\n",
    "        self.args = args\n",
    "        \n",
    "        if refmod:\n",
    "            self.refmod = refmod\n",
    "        else:\n",
    "            def zr(x):\n",
    "                return 0\n",
    "            self.refmod = zr\n",
    "            \n",
    "    def LCB(self, x):\n",
    "        x = np.array([x]).reshape(-1,1);\n",
    "        x = x.reshape(int(x.shape[0]/self.dim), self.dim)\n",
    "        \n",
    "        mu, std = self.model.predict(x, return_std=True);\n",
    "        mu = mu.flatten()\n",
    "        \n",
    "        yref = self.refmod(self.descale(x), *self.args)\n",
    "            \n",
    "        return (yref+mu-self.exp_w*std).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SYST_RECYCLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set system and reference functions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m SYST_RECYCLE \u001b[38;5;241m=\u001b[39m \u001b[43mSYST_RECYCLE\u001b[49m\n\u001b[0;32m      3\u001b[0m SYST_RECYCLE_REF \u001b[38;5;241m=\u001b[39m RXTR_REG()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train reference model (GP)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SYST_RECYCLE' is not defined"
     ]
    }
   ],
   "source": [
    "# Set system and reference functions\n",
    "SYST_RECYCLE = SYST_RECYCLE\n",
    "SYST_RECYCLE_REF = RXTR_REG()\n",
    "\n",
    "\n",
    "# Train reference model (GP)\n",
    "\n",
    "TT = np.arange(303, 424, 1)\n",
    "TT = np.meshgrid(TT, TT)\n",
    "TT = np.hstack([TT[0].reshape(-1, 1), TT[1].reshape(-1, 1)])\n",
    "\n",
    "print('Create Statistical Reference Model...')\n",
    "start = time.time()\n",
    "\n",
    "Tmod = np.linspace(303, 423, 13)\n",
    "Tmod = np.meshgrid(Tmod, Tmod)\n",
    "Tmod = np.hstack([Tmod[0].reshape(-1, 1), Tmod[1].reshape(-1, 1)])\n",
    "CTREF = np.ones((Tmod.shape[0], 3))\n",
    "\n",
    "Ctref = Parallel(n_jobs = 5)(delayed(SYST_RECYCLE_REF)(Tmod, FR, R_Frac, Cdist)\n",
    "                             for Cdist in C0varf)\n",
    "\n",
    "for i in range(3):\n",
    "    C = np.vstack(Ctref[:][:]).T[:, i::3]\n",
    "    C = 8*np.sum(C, axis = 1)\n",
    "    CTREF[:, i] = C\n",
    "\n",
    "kergp = gpr.kernels.Matern((5, 5), ((1, 10), (1, 10)), nu = 2.5)\n",
    "gprefmod = gpr.GaussianProcessRegressor(kernel = kergp,\n",
    "                                        alpha = 1e-6,\n",
    "                                        n_restarts_optimizer = 10,\n",
    "                                        normalize_y = True)\n",
    "gprefmod.fit(scale(Tmod), CTREF)\n",
    "\n",
    "end = time.time()\n",
    "mobdtm = end-start\n",
    "print(mobdtm)\n",
    "\n",
    "def SYST_C(T):\n",
    "    CtR = 0\n",
    "    for i in range(C0varf.shape[0]):\n",
    "        Ctr = 8*SYST_RECYCLE(T, FR, R_Frac, C0varf[i])[-1]\n",
    "        CtR += Ctr\n",
    "    return CtR\n",
    "\n",
    "def SYST_C_DIST(T):\n",
    "    CtR = 0\n",
    "    for i in range(C0varf.shape[0]):\n",
    "        Ctr = SYST_RECYCLE(T, FR, R_Frac, C0varf[i])\n",
    "        Ctr = np.vstack(Ctr[:]).T\n",
    "        CtR += 8*Ctr\n",
    "    return CtR\n",
    "\n",
    "def SYST_C_REFGP(T):\n",
    "    T = T.flatten()\n",
    "    T = T.reshape(int(T.shape[0]/2), 2)\n",
    "    T = scale(T)\n",
    "    return gprefmod.predict(T)[:, -1]\n",
    "\n",
    "def SYST_C_DISTGP(T):\n",
    "    T = T.flatten()\n",
    "    T = T.reshape(int(T.shape[0]/2), 2)\n",
    "    T = scale(T)\n",
    "    return gprefmod.predict(T)\n",
    "\n",
    "def SYST_C_REF1(T):\n",
    "    T = T.flatten()\n",
    "    T = T.reshape(int(T.shape[0]/2), 2)\n",
    "    T = scale(T)\n",
    "    return gprefmod.predict(T)[:, 0]\n",
    "\n",
    "def SYST_C_REF2(T):\n",
    "    T = T.flatten()\n",
    "    T = T.reshape(int(T.shape[0]/2), 2)\n",
    "    T = scale(T)\n",
    "    return gprefmod.predict(T)[:, 1]\n",
    "\n",
    "def zr(x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    x = x.reshape(int(x.shape[0]/2), 2)\n",
    "    return np.zeros((x.shape[0], 3))\n",
    "\n",
    "\n",
    "# Setup BO class\n",
    "REACOPTIM = BO_algos.BO(ub = ub,\n",
    "                        lb = lb,\n",
    "                        dim = 2,\n",
    "                        exp_w = exp_w,\n",
    "                        kernel = kernel,\n",
    "                        system = SYST_C,\n",
    "                        bounds = bounds,\n",
    "                        **{'refmod': SYST_C_REFGP,\n",
    "                           'distmod': SYST_C_DIST,\n",
    "                           'ref_distmod': SYST_C_DISTGP,\n",
    "                           'ref_distmod1': SYST_C_REF1,\n",
    "                           'ref_distmod2': SYST_C_REF2})\n",
    "\n",
    "\n",
    "## Generate level sets\n",
    "gpparts = gpr.GaussianProcessRegressor(kernel = kergp,\n",
    "                                       alpha = 1e-6,\n",
    "                                       n_restarts_optimizer = 10,\n",
    "                                       normalize_y = True)\n",
    "gpparts.fit(scale(Tmod), (CTREF[:, -1]).reshape(-1, 1))\n",
    "\n",
    "parts = np.array([-461, -383]) # ref mod with g_1(T_1) and g_2(T_1, T_2)\n",
    "\n",
    "con1 = lambda x: (gpparts.predict(x.reshape(1, 2))).flatten()\n",
    "con2 = lambda x: x[0]\n",
    "\n",
    "nlc1 = NonlinearConstraint(con1, -1e4, parts[0])\n",
    "nlc21 = NonlinearConstraint(con1, parts[0], parts[1])\n",
    "nlc22 = NonlinearConstraint(con2, 0.4, 1.1)\n",
    "nlc31 = NonlinearConstraint(con1, parts[0], parts[1])\n",
    "nlc32 = NonlinearConstraint(con2, 0, 0.38)\n",
    "nlc4 = NonlinearConstraint(con1, parts[1], 1e4)\n",
    "\n",
    "cons = {'1': [nlc1], '2': [nlc21, nlc22], '3': [nlc31, nlc32], '4': [nlc4]}\n",
    "\n",
    "## VP-BO intial points\n",
    "lim_init = REACOPTIM.scale(np.array([336, 380]))\n",
    "\n",
    "\n",
    "# Run_BO\n",
    "trials_seq = 100\n",
    "trials_par1 = 25\n",
    "trials_par2 = 33\n",
    "\n",
    "x_init = np.linspace(bounds.lb, bounds.ub, 5)\n",
    "x_init = np.meshgrid(*x_init.T)\n",
    "x_init = np.reshape(x_init, (dim, -1)).T\n",
    "\n",
    "MET_SBO = np.zeros((trials_seq, 4))\n",
    "PARAMS_SBO = np.ones(x_init.shape)\n",
    "DIST_SBO = np.array([]).reshape(0, dim)\n",
    "RES_SBO = np.ones((MET_SBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_REFBO = np.zeros((trials_seq, 4))\n",
    "PARAMS_REFBO = np.ones(x_init.shape)\n",
    "DIST_REFBO = np.array([]).reshape(0, dim)\n",
    "RES_REFBO = np.ones((MET_REFBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_LSBO1 = np.zeros((trials_par1, 4))\n",
    "PARAMS_LSBO1 = np.ones(x_init.shape)\n",
    "DIST_LSBO1 = np.array([]).reshape(0, dim)\n",
    "RES_LSBO1 = np.ones((MET_LSBO1.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_LSBO2 = np.zeros((trials_par1, 4))\n",
    "PARAMS_LSBO2 = np.ones(x_init.shape)\n",
    "DIST_LSBO2 = np.array([]).reshape(0, dim)\n",
    "RES_LSBO2 = np.ones((MET_LSBO2.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_VPBO1 = np.zeros((trials_par2, 4))\n",
    "PARAMS_VPBO1 = np.ones(x_init.shape)\n",
    "DIST_VPBO1 = np.array([]).reshape(0, dim)\n",
    "RES_VPBO1 = np.ones((MET_VPBO1.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_VPBO2 = np.zeros((trials_par2, 4))\n",
    "PARAMS_VPBO2 = np.ones(x_init.shape)\n",
    "DIST_VPBO2 = np.array([]).reshape(0, dim)\n",
    "RES_VPBO2 = np.ones((MET_VPBO2.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_HSBO = np.zeros((trials_par1, 4))\n",
    "PARAMS_HSBO = np.ones(x_init.shape)\n",
    "DIST_HSBO = np.array([]).reshape(0, dim)\n",
    "RES_HSBO = np.ones((MET_HSBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_EXBO = np.zeros((trials_par1, 4))\n",
    "PARAMS_EXBO = np.ones(x_init.shape)\n",
    "DIST_EXBO = np.array([]).reshape(0, dim)\n",
    "RES_EXBO = np.ones((MET_EXBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_NMCBO = np.zeros((trials_par1, 4))\n",
    "PARAMS_NMCBO = np.ones(x_init.shape)\n",
    "DIST_NMCBO = np.array([]).reshape(0, dim)\n",
    "RES_NMCBO = np.ones((MET_NMCBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "MET_QBO = np.zeros((trials_par1, 4))\n",
    "PARAMS_QBO = np.ones(x_init.shape)\n",
    "DIST_QBO = np.array([]).reshape(0, dim)\n",
    "RES_QBO = np.ones((MET_QBO.shape[0], x_init.shape[0]))\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "## VP-BO \n",
    "for i, x_0 in enumerate(x_init):\n",
    "    start = time.time()\n",
    "    optpltvar(trials = trials_par2,\n",
    "                             split_num = 2,\n",
    "                             lim_init = lim_init,\n",
    "                             f_cores = 3,\n",
    "                             af_cores = 1,\n",
    "                             ref_cores = 1,\n",
    "                             x_init  = x_0)\n",
    "    end = time.time()\n",
    "    print('Run time '+str(end-start)+'s')\n",
    "    print('iteration '+str(i+1))\n",
    "    MET_VPBO1[:, 0] += time_vp.flatten()\n",
    "    MET_VPBO1[:, 1] += REACOPTIM.time_fvp.flatten()\n",
    "    MET_VPBO1[:, 2] += REACOPTIM.y_vpbst[:, -1].flatten()\n",
    "    MET_VPBO1[:, 3] += np.min(REACOPTIM.y_vpbst[:, -1])\n",
    "    print('Best VP-BO value is '+str(np.min(y_vpbst[:, -1])))\n",
    "    PARAMS_VPBO1[i] = REACOPTIM.x_vp[np.argmin(REACOPTIM.y_vp[:, -1])]\n",
    "    DIST_VPBO1 = np.vstack([DIST_VPBO1, REACOPTIM.x_vp])\n",
    "    RES_VPBO1[:, i] = REACOPTIM.y_vpbst[:, -1]\n",
    "MET_VPBO1[:, 0] = MET_VPBO1[:, 0]/(i+1)\n",
    "MET_VPBO1[:, 1] = MET_VPBO1[:, 1]/(i+1)\n",
    "MET_VPBO1[:, 2] = MET_VPBO1[:, 2]/(i+1)\n",
    "MET_VPBO1[:, 3] = MET_VPBO1[:, 3]/(i+1)\n",
    "REACOPTIM.y_vpbst = MET_VPBO1[:, 2].reshape(-1, 1)\n",
    "REACOPTIM.time_vp = MET_VPBO1[:, 0].flatten()\n",
    "REACOPTIM.time_fvp = MET_VPBO1[:, 1].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2\n",
    "dist_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optpltvar(trials, split_num, liminit, scaling_factor, fcores, afcores, xinit  = None):\n",
    "        \"\"\"\n",
    "        VP-BO approach...\n",
    "        The partitions are made using the 'split_num' argument; this sets the number of blocks that the variables\n",
    "        are split into. \n",
    "        The 'liminit' argument sets the initial value for the x_{-k} variables.\n",
    "        Note that when introducing the design variables, x, they should be ordered by subsystem.\n",
    "        For example if d = 4, and split_num = 2, introducing x = (x_a, x_b, x_c, x_d) will set\n",
    "        x_1 = (x_a, x_b) and x_{-1}] = (x_c, x_d), and x_2 = (x_c, x_d) and x_{-2} = (x_a, x_b).\n",
    "        The 'fcores' argument sets the number of cores used for parallel experimentation, and\n",
    "        the 'afcores' argument sets the cores used for optimizing the AF\n",
    "        \"\"\"\n",
    "        print('Partitioned Variables BO Run...')\n",
    "        start = time.time()\n",
    "        trialspltvar = trials\n",
    "        splitvar = split_num\n",
    "        timespltvar = np.zeros(trialspltvar+1)\n",
    "        timefspltvar = np.zeros(trialspltvar+1)\n",
    "        div = int(dim/splitvar)\n",
    "        sf = scaling_factor\n",
    "        refmod = dist_ref['distrefmod']\n",
    "\n",
    "        x = liminit*np.ones((splitvar, dim))\n",
    "        lwr = x.copy()\n",
    "        upr = x.copy()+1e-6\n",
    "        \n",
    "        for i in range(splitvar):\n",
    "            if xinit is None:\n",
    "                x[i, i*div:(i+1)*div] = np.random.uniform(0, 1, (1, div))\n",
    "            else:\n",
    "                xinit = xinit.reshape(1, dim)\n",
    "                x[i, i*div:(i+1)*div] = xinit[0, i*div:(i+1)*div]\n",
    "            lwr[i, i*div:(i+1)*div] = 0\n",
    "            upr[i, i*div:(i+1)*div] = sf\n",
    "        x = np.vstack([x, liminit])\n",
    "        splt = int(x.shape[0]/fcores)\n",
    "        xbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "        \n",
    "        if fcores == 1:\n",
    "            xbs[0] = x\n",
    "        else:\n",
    "            for i in range(fcores-1):\n",
    "                xbs[i] = x[i*splt:(i+1)*splt, :]\n",
    "            xbs[-1] = x[(i+1)*splt:, :]\n",
    "\n",
    "            \n",
    "        startf = time.time()\n",
    "        y = Parallel(n_jobs = fcores)(delayed(distmod)((start_point)) for start_point in xbs)\n",
    "        if str(type(refmod))==\"<class '__main__.Network'>\":\n",
    "            yref = Parallel(n_jobs = fcores)(delayed(refmod)(start_point) for start_point in torch.from_numpy(x).float())\n",
    "            endf = time.time()\n",
    "            yref = torch.hstack(yref[:]).T.reshape(-1, 1).data.numpy()\n",
    "        else:\n",
    "            yref = Parallel(n_jobs = fcores)(delayed(refmod)((start_point)) for start_point in xbs)\n",
    "            endf = time.time()\n",
    "            yref = np.vstack(yref[:])\n",
    "        timefspltvar[0] = endf-startf\n",
    "        y = np.vstack(y[:])\n",
    "        eps = y-yref\n",
    "        ybst = np.min(y, axis = 0).reshape(-1, 1).T\n",
    "        modelspltvar = {}\n",
    "        bndsvar = {}\n",
    "        LCB = {}\n",
    "        xnxt = x.copy()\n",
    "        init_pts = int(round(128**(div/dim)))\n",
    "        x0 = np.random.uniform(0, sf, (init_pts, dim))\n",
    "        for i in range(splitvar):\n",
    "            modelspltvar[str(i+1)] = gpr.GaussianProcessRegressor(kernel, alpha = 1e-6,\n",
    "                                                                  n_restarts_optimizer = 10,\n",
    "                                                                  normalize_y = True)\n",
    "            modelspltvar[str(i+1)].fit(x, eps[:, i])\n",
    "            bndsvar[str(i+1)] = Bounds(lwr[i], upr[i])\n",
    "            LCB[str(i+1)] = LCB_AF(modelspltvar[str(i+1)], dim, exp_w,\n",
    "                                   descale, **{'refmod': dist_ref['distrefmod'+str(i+1)]}).LCB\n",
    "            opt = Parallel(n_jobs = afcores)(delayed(minimize)(LCB[str(i+1)], x0 = start_point,\n",
    "                                                              method = 'L-BFGS-B',\n",
    "                                                              bounds = bndsvar[str(i+1)])\n",
    "                                            for start_point in x0)\n",
    "            xnxts = np.array([res.x for res in opt], dtype = 'float')\n",
    "            funs = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "            xnxt[i] = xnxts[np.argmin(funs)]\n",
    "            xnxt[-1, i*div:(i+1)*div] = xnxts[np.argmin(funs), i*div:(i+1)*div]\n",
    "        xnxtbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "        end = time.time()\n",
    "        timespltvar[0] = end-start\n",
    "        for i in range(trialspltvar):\n",
    "            if fcores == 1:\n",
    "                xnxtbs[0] = xnxt\n",
    "            else:\n",
    "                for j in range(fcores-1):\n",
    "                    xnxtbs[j] = xnxt[j*splt:(j+1)*splt, :]\n",
    "                xnxtbs[-1] = xnxt[(j+1)*splt:, :]\n",
    "            startf = time.time()\n",
    "            ynxt = Parallel(n_jobs = fcores)(delayed(distmod)(descale(start_point)) for start_point in xnxtbs)\n",
    "            if str(type(refmod))==\"<class '__main__.Network'>\":\n",
    "                yref = Parallel(n_jobs = fcores)(delayed(refmod)(start_point) for start_point in torch.from_numpy(x).float())\n",
    "                endf = time.time()\n",
    "                yref = torch.hstack(yref[:]).T.reshape(-1, 1).data.numpy()\n",
    "            else:\n",
    "                yref = Parallel(n_jobs = fcores)(delayed(refmod)(descale(start_point)) for start_point in xnxtbs)\n",
    "                endf = time.time()\n",
    "                yref = np.vstack(yref[:])\n",
    "            ynxt = np.vstack(ynxt[:])\n",
    "            epsnxt = ynxt-yref\n",
    "            timefspltvar[i+1] = timefspltvar[i]+(endf-startf)\n",
    "            for j in range(splitvar):\n",
    "                if any(ynxt[:, j] < min(y[:, j])):\n",
    "                    lwr[j] = xnxt[np.argmin(ynxt[:, j])]\n",
    "                    lwr[j, j*div:(j+1)*div] = 0\n",
    "                    upr[j] = xnxt[np.argmin(ynxt[:, j])]+1e-6\n",
    "                    upr[j, j*div:(j+1)*div] = sf\n",
    "            x = np.vstack([x, xnxt])\n",
    "            y = np.vstack([y, ynxt])\n",
    "            eps = np.vstack([eps, epsnxt])\n",
    "            ybst = np.vstack([ybst, np.min(ynxt, axis = 0).reshape(-1,1).T])\n",
    "            x0 = np.random.uniform(0, sf, (init_pts, dim))\n",
    "            for j in range(splitvar):\n",
    "                modelspltvar[str(j+1)].fit(x, eps[:, j])\n",
    "                bndsvar[str(j+1)] = Bounds(lwr[j], upr[j])\n",
    "                opt = Parallel(n_jobs = afcores)(delayed(minimize)(LCB[str(j+1)], x0 = start_point,\n",
    "                                                                  method = 'L-BFGS-B',\n",
    "                                                                  bounds = bndsvar[str(j+1)])\n",
    "                                                for start_point in x0)\n",
    "                xnxts = np.array([res.x for res in opt], dtype = 'float')\n",
    "                funs = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "                xnxt[j] = xnxts[np.argmin(funs)]\n",
    "                xnxt[-1, j*div:(j+1)*div] = xnxts[np.argmin(funs), j*div:(j+1)*div]\n",
    "            end = time.time()\n",
    "            timespltvar[i+1] = end-start\n",
    "        spltvar_optim = True\n",
    "        modelspltvar = modelspltvar\n",
    "        xspltvar = descale(x)\n",
    "        yspltvar = y\n",
    "        yspltvarbst = ybst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = x_train.shape[1]\n",
    "kernel = gpr.kernels.Matern(np.ones(sz), np.array([k_bnds[i]]*sz), nu = nu[i])\n",
    "dim = x_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = None, mu = None, sigma = None):\n",
    "    if mu is None:\n",
    "        mu = np.mean(x, axis = axis)\n",
    "        sigma = np.std(x, axis = axis, ddof = 1)\n",
    "        return (x-mu)/sigma, mu, sigma\n",
    "    \n",
    "    else:\n",
    "        return (x-mu)/sigma\n",
    "\n",
    "\n",
    "def unnormalize(x, mu, sigma):\n",
    "    return sigma*x+mu\n",
    "\n",
    "def descale(self, x):\n",
    "    m = (self.ub-self.lb)/(self.bounds.ub-self.bounds.lb)\n",
    "    b = self.ub-m*self.bounds.ub\n",
    "    return m*x+b\n",
    "\n",
    "\n",
    "def scale(self, x, use_self = True, lb = None, ub = None):\n",
    "    if use_self:    \n",
    "        m = (self.bounds.ub-self.bounds.lb)/(self.ub-self.lb)\n",
    "        b = self.bounds.ub-m*self.ub\n",
    "    else:\n",
    "        m = (self.bounds.ub-self.bounds.lb)/(ub-lb)\n",
    "        b = self.bounds.ub-m*ub\n",
    "    return m*x+b\n",
    "\n",
    "if aux_mods:\n",
    "    refmod = list(aux_mods.values())[0]\n",
    "    if len(aux_mods) > 1:\n",
    "        distmod = aux_mods['distmod']\n",
    "        dist_ref['distrefmod'] = aux_mods['ref_distmod']\n",
    "        for i in range(3,len(aux_mods)):\n",
    "            dist_ref['distrefmod'+str(i-2)] = aux_mods['ref_distmod'+str(i-2)]\n",
    "        dist_ref = OrderedDict(dist_ref)\n",
    "\n",
    "bois_optim = False\n",
    "mcbo_optim = False\n",
    "opbo_optim = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimvpbo(trials, fcores ):\n",
    "    start= time.time()\n",
    "    trialspltvar = trials\n",
    "    splitvar = split_num\n",
    "    timespltvar= np.zeros(trialspltvar+1)\n",
    "    timefspltvar = np.zeros(trialspltvar+1)\n",
    "    div = int(dim/splitvar)\n",
    "    sf = scaling_factor\n",
    "    refmod = np.array([]).reshape(0, dim)\n",
    "\n",
    "    x = liminit*np.ones((splitvar, dim))\n",
    "    lwr = x.copy()\n",
    "    upr = x.copy()+1e-6\n",
    "\n",
    "        \n",
    "    for i in range(splitvar):\n",
    "        if xinit is None:\n",
    "            x[i, i*div:(i+1)*div] = np.random.uniform(0, 1, (1, div))\n",
    "        else:\n",
    "            xinit = xinit.reshape(1, dim)\n",
    "            x[i, i*div:(i+1)*div] = xinit[0, i*div:(i+1)*div]\n",
    "        lwr[i, i*div:(i+1)*div] = 0\n",
    "        upr[i, i*div:(i+1)*div] = sf\n",
    "        x = np.vstack([x, liminit])\n",
    "        splt = int(x.shape[0]/fcores)\n",
    "        xbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "\n",
    "\n",
    "        if fcores == 1:\n",
    "            xbs[0] = x\n",
    "        else:\n",
    "            for i in range(fcores-1):\n",
    "                xbs[i] = x[i*splt:(i+1)*splt, :]\n",
    "            xbs[-1] = x[(i+1)*splt:, :]\n",
    "\n",
    "        startf = time.time()  \n",
    "    y = Parallel(n_jobs = fcores)(delayed(distmod)((start_point)) for start_point in xbs)\n",
    "    if str(type(refmod))==\"<class '__main__.Network'>\":\n",
    "        yref = Parallel(n_jobs = fcores)(delayed(refmod)(start_point) for start_point in torch.from_numpy(x).float())\n",
    "        endf = time.time()\n",
    "        yref = torch.hstack(yref[:]).T.reshape(-1, 1).data.numpy()      \n",
    "    else:\n",
    "        yref = Parallel(n_jobs = fcores)(delayed(refmod)((start_point)) for start_point in xbs)\n",
    "        endf = time.time()\n",
    "        yref = np.vstack(yref[:])\n",
    "    timefspltvar[0] = endf-startf\n",
    "    y = np.vstack(y[:])\n",
    "    eps = y-yref\n",
    "    ybst = np.min(y, axis = 0).reshape(-1, 1).T\n",
    "    modelspltvar = {}\n",
    "    bndsvar = {}\n",
    "    LCB = {}\n",
    "    xnxt = x.copy()\n",
    "    init_pts = int(round(128**(div/dim)))\n",
    "    x0 = np.random.uniform(0, sf, (init_pts, dim))\n",
    "\n",
    "\n",
    "    for i in range(splitvar):\n",
    "        modelspltvar[str(i+1)] = gpr.GaussianProcessRegressor(kernel, alpha = 1e-6,\n",
    "                                                                  n_restarts_optimizer = 10,\n",
    "                                                                  normalize_y = True)\n",
    "        modelspltvar[str(i+1)].fit(x, eps[:, i])\n",
    "        bndsvar[str(i+1)] = Bounds(lwr[i], upr[i])\n",
    "        LCB[str(i+1)] = LCB_AF(modelspltvar[str(i+1)], dim, exp_w,\n",
    "                                   descale, **{'refmod': dist_ref['distrefmod'+str(i+1)]}).LCB\n",
    "        opt = Parallel(n_jobs = afcores)(delayed(minimize)(LCB[str(i+1)], x0 = start_point,\n",
    "                                                        method = 'L-BFGS-B',\n",
    "                                                        bounds = bndsvar[str(i+1)])   \n",
    "                                        for start_point in x0)   \n",
    "        xnxts = np.array([res.x for res in opt], dtype = 'float')\n",
    "        funs = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "        xnxt[i] = xnxts[np.argmin(funs)]\n",
    "        xnxt[-1, i*div:(i+1)*div] = xnxts[np.argmin(funs), i*div:(i+1)*div]\n",
    "    xnxtbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "    end = time.time()\n",
    "    timespltvar[0] = end-start\n",
    "    for i in range(trialspltvar):\n",
    "        if fcores == 1:\n",
    "            xnxtbs[0] = xnxt\n",
    "        else:\n",
    "            for j in range(fcores-1):\n",
    "                xnxtbs[j] = xnxt[j*splt:(j+1)*splt, :]\n",
    "            xnxtbs[-1] = xnxt[(j+1)*splt:, :]\n",
    "        startf = time.time()\n",
    "        ynxt = Parallel(n_jobs = fcores)(delayed(distmod)(descale(start_point)) for start_point in xnxtbs)\n",
    "        if str(type(refmod))==\"<class '__main__.Network'>\":\n",
    "            yref = Parallel(n_jobs = fcores)(delayed(refmod)(start_point) for start_point in torch.from_numpy(x).float())\n",
    "            endf = time.time()\n",
    "            yref = torch.hstack(yref[:]).T.reshape(-1, 1).data.numpy()\n",
    "        else:\n",
    "            yref = Parallel(n_jobs = fcores)(delayed(refmod)(descale(start_point)) for start_point in xnxtbs)\n",
    "            endf = time.time()\n",
    "            yref = np.vstack(yref[:])\n",
    "        ynxt = np.vstack(ynxt[:])\n",
    "        epsnxt = ynxt-yref\n",
    "        timefspltvar[i+1] = timefspltvar[i]+(endf-startf)\n",
    "        \n",
    "        for j in range(splitvar):\n",
    "            if any(ynxt[:, j] < min(y[:, j])):\n",
    "                lwr[j] = xnxt[np.argmin(ynxt[:, j])]\n",
    "                lwr[j, j*div:(j+1)*div] = 0\n",
    "                upr[j] = xnxt[np.argmin(ynxt[:, j])]+1e-6\n",
    "                upr[j, j*div:(j+1)*div] = sf\n",
    "        x = np.vstack([x, xnxt])\n",
    "        y = np.vstack([y, ynxt])\n",
    "        eps = np.vstack([eps, epsnxt])\n",
    "        ybst = np.vstack([ybst, np.min(ynxt, axis = 0).reshape(-1,1).T])\n",
    "        x0 = np.random.uniform(0, sf, (init_pts, dim))\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(splitvar):\n",
    "            modelspltvar[str(j+1)].fit(x, eps[:, j])\n",
    "            bndsvar[str(j+1)] = Bounds(lwr[j], upr[j])\n",
    "            opt = Parallel(n_jobs = afcores)(delayed(minimize)(LCB[str(j+1)], x0 = start_point,\n",
    "                                                                method = 'L-BFGS-B',\n",
    "                                                                bounds = bndsvar[str(j+1)])\n",
    "                                            for start_point in x0)\n",
    "            xnxts = np.array([res.x for res in opt], dtype = 'float')\n",
    "            funs = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "            xnxt[j] = xnxts[np.argmin(funs)]\n",
    "            xnxt[-1, j*div:(j+1)*div] = xnxts[np.argmin(funs), j*div:(j+1)*div]\n",
    "        end = time.time()\n",
    "        timespltvar[i+1] = end-start\n",
    "    spltvar_optim = True\n",
    "    modelspltvar = modelspltvar\n",
    "    xspltvar = descale(x)\n",
    "    yspltvar = y\n",
    "    yspltvarbst = ybst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VP-BO \n",
    "for i, x_0 in enumerate(x_init):\n",
    "    start = time.time()\n",
    "    optpltvar(trials = trials_par2,\n",
    "                             split_num = 2,\n",
    "                             lim_init = lim_init,\n",
    "                             f_cores = 3,\n",
    "                             af_cores = 1,\n",
    "                             ref_cores = 1,\n",
    "                             x_init  = x_0)\n",
    "    end = time.time()\n",
    "    print('Run time '+str(end-start)+'s')\n",
    "    print('iteration '+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpectedImprovement\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_acqf\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m euclidean\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botorch\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgp_settings\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlinear_operator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlinop_settings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     acquisition,\n\u001b[0;32m     11\u001b[0m     exceptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     test_functions,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gpytorch\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Union\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlinear_operator\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlinear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\linear_operator\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m beta_features, operators, settings, utils\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     add_diagonal,\n\u001b[0;32m      5\u001b[0m     add_jitter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     sqrt_inv_matmul,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator, to_dense, to_linear_operator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\linear_operator\\beta_features.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _feature_flag\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_moved_beta_feature\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_cls, orig_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\linear_operator\\settings.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_dtype_value_context\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     _global_float_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:533\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    535\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saadr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTaskGP\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Custom function that creates a GP and mll object from data and model settings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gp\u001b[39m(\n\u001b[0;32m      8\u001b[0m     X: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m      9\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     10\u001b[0m     mean_module: gpytorch\u001b[38;5;241m.\u001b[39mmeans\u001b[38;5;241m.\u001b[39mMean,\n\u001b[0;32m     11\u001b[0m     covar_module: gpytorch\u001b[38;5;241m.\u001b[39mkernels\u001b[38;5;241m.\u001b[39mKernel,\n\u001b[0;32m     12\u001b[0m     likelihood: gpytorch\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mLikelihood\n\u001b[1;32m---> 13\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[\n\u001b[0;32m     14\u001b[0m     gpytorch\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mGP,\n\u001b[0;32m     15\u001b[0m     gpytorch\u001b[38;5;241m.\u001b[39mmlls\u001b[38;5;241m.\u001b[39mMarginalLogLikelihood\n\u001b[0;32m     16\u001b[0m ]:\n\u001b[0;32m     17\u001b[0m     gp \u001b[38;5;241m=\u001b[39m SingleTaskGP(\n\u001b[0;32m     18\u001b[0m         X, y,\n\u001b[0;32m     19\u001b[0m         mean_module\u001b[38;5;241m=\u001b[39mmean_module,\n\u001b[0;32m     20\u001b[0m         covar_module\u001b[38;5;241m=\u001b[39mcovar_module,\n\u001b[0;32m     21\u001b[0m         likelihood\u001b[38;5;241m=\u001b[39mlikelihood\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m     mll \u001b[38;5;241m=\u001b[39m ExactMarginalLogLikelihood(gp\u001b[38;5;241m.\u001b[39mlikelihood, gp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "# Custom function that creates a GP and mll object from data and model settings\n",
    "def get_gp(\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    mean_module: gpytorch.means.Mean,\n",
    "    covar_module: gpytorch.kernels.Kernel,\n",
    "    likelihood: gpytorch.likelihoods.Likelihood\n",
    ") -> Tuple[\n",
    "    gpytorch.models.GP,\n",
    "    gpytorch.mlls.MarginalLogLikelihood\n",
    "]:\n",
    "    gp = SingleTaskGP(\n",
    "        X, y,\n",
    "        mean_module=mean_module,\n",
    "        covar_module=covar_module,\n",
    "        likelihood=likelihood\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    return gp, mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liminit= np.ones((1, 9))\n",
    "liminit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([336, 380])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''FR = np.array([0.100, 0.075, 0.075])\n",
    "R_Frac = 1e-6\n",
    "ub = np.array([423, 423])\n",
    "lb = np.array([303, 303])\n",
    "lim_init = (np.array([336, 380]))\n",
    "lim_init'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FR = np.array([0.100, 0.075, 0.075])\n",
    "R_Frac = 1e-6\n",
    "ub = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "lb = np.array([0, 0, 0, 0, 0 , 0, 0, 0, 0])\n",
    "lim_init = (np.array([0, 0, 0, 0, 0 , 0, 0, 0, 0]))\n",
    "lim_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = Bounds((0, 0, 0, 0, 0, 0, 0, 0, 0), (1, 1, 1,1, 1, 1,1, 1, 1))\n",
    "dim = len(ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds, NonlinearConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ],\n",
       "       [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75],\n",
       "       [1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_init = np.linspace(bounds.lb, bounds.ub, 5)\n",
    "x_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Bounds' has no attribute 'lb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mBounds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlb\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Bounds' has no attribute 'lb'"
     ]
    }
   ],
   "source": [
    "Bounds.lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.275     ,  0.64166667, -0.025     , -0.025     , -2.525     ,\n",
       "       -2.525     , -0.025     , -0.025     , -0.85833333])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lim_init = scale(np.array([336, 380, 300, 300, 000, 000,300,300,200]))\n",
    "lim_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reference model (GP)\n",
    "def scale(x, ub = 423, lb = 303, sf = 1):\n",
    "    m = sf/(ub - lb)\n",
    "    b = -lb*m\n",
    "    return m*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = liminit*np.ones((1, 9))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.vstack([x, lim_init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 0.275     ,  0.64166667, -0.025     , -0.025     , -2.525     ,\n",
       "        -2.525     , -0.025     , -0.025     , -0.85833333]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_pts = int(len(x)/2)\n",
    "init_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 9), dtype=float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refmod = np.array([]).reshape(0, dim)\n",
    "refmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restarts = int(round(128/(splits+1), 0))\n",
    "restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 101)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:101\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def optim_vpbo(trials, split_num, liminit, fcores=1, afcores=1, refcores=1, x_init=x): \n",
    "    start= time.time()\n",
    "    trialspltvar = trials\n",
    "    splitvar = split_num\n",
    "    time_vp= np.zeros(trialspltvar+1)\n",
    "    time_fvp = np.zeros(trialspltvar+1)\n",
    "    div = int(dim/splitvar)\n",
    "    #sf = scaling_factor\n",
    "    refmod = np.array([]).reshape(0, dim)\n",
    "    \n",
    "    x = liminit*np.ones((splitvar, dim))\n",
    "    lwr = x.copy()\n",
    "    upr = x.copy()+1e-6\n",
    "\n",
    "    for i in range(splitvar):\n",
    "        if x_init is None:\n",
    "            x[i, i*div:(i+1)*div] = np.random.uniform(  bounds.lb,\n",
    "                                                        bounds.ub,\n",
    "                                                        (1, div))\n",
    "        else:\n",
    "            x_init = x_init.reshape(1, dim)\n",
    "            x[i, i*div:(i+1)*div] = x_init[0, i*div:(i+1)*div]       \n",
    "        \n",
    "        lwr[i, i*div:(i+1)*div] = bounds.lb[i]\n",
    "        upr[i, i*div:(i+1)*div] = bounds.ub[i]\n",
    "    x = np.vstack([x, liminit])\n",
    "        \n",
    "    init_pts = int(len(x)/splitvar)\n",
    "    splt = int(x.shape[0]/fcores)\n",
    "    xbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "\n",
    "    \n",
    "\n",
    "#results = Parallel(n_jobs=fcores)(delayed(run_trial)(params) for params in param_combinations[:trials])\n",
    "\n",
    "    if fcores == 1:\n",
    "        xbs[0] = x\n",
    "    else:\n",
    "        for i in range(fcores-1):\n",
    "            xbs[i] = x[i*splt:(i+1)*splt, :]\n",
    "        xbs[-1] = x[(i+1)*splt:, :]\n",
    "\n",
    "    start_f =time.time()\n",
    "    y= Parallel(n_jobs= fcores)(delayed(descale(x_s) for x_s in xbs))\n",
    "    \n",
    "    #y_ref=y_test\n",
    "    \n",
    "    y_test =Parallel(n_jobs = refcores)(delayed(descale(x_s) for x_s in xbs))\n",
    "    y_test = np.vstack(y_test[:])\n",
    "    end_f = time.time() \n",
    "    time_fvp[0] = end_f-start_f\n",
    "      \n",
    "    y = np.vstack(y[:])\n",
    "    eps = y-y_ref\n",
    "    y_best= np.min(y,azis = 0).reshape(-1, 1).T\n",
    "\n",
    "    mll = {}\n",
    "    gp = {}\n",
    "    acq_fun = {}\n",
    "\n",
    "# Simulate BO\n",
    "    for i in range(splitvar):\n",
    "        # 1. Create a new GP with the best setup \n",
    "        gp[str(i+1)], mll[str(i+1)] = get_gp(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            ConstantMean(),\n",
    "            ScaleKernel(RBFKernel()),\n",
    "            GaussianLikelihood()\n",
    "        )\n",
    "        \n",
    "        mll[str(i+1)] = fit_gpytorch_mll(mll)\n",
    "        bnds_var[str(i+1)] = Bounds(lwr[i], upr[i])\n",
    "        # 2. Define and optimize an acquisition function\n",
    "        acq_fun[str(i+1)] = ExpectedImprovement(model=gp[str(i+1)], \n",
    "                                                best_f=torch.min(y_train))\n",
    "\n",
    "    restarts = int(round(128/(splits+1), 0))\n",
    "    X_next = x.copy()\n",
    "    X_test_best = np.array(np.ones(f_cores), dtype = tuple)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i in range(init_pts):\n",
    "        self.time_vp[i] = (i+1)*(end-start)/init_pts\n",
    "        self.time_fvp[i] = (i+1)*(end_f-start_f)/init_pts\n",
    "    print('ITERATION COUNT IS AT 'f'{init_pts};\\\n",
    "        TOTAL ELAPSED TIME: 'f'{self.time_vp[init_pts-1]:.1f}')\n",
    "\n",
    "    end = time.time()\n",
    "    self.time_vp[0] = end-start\n",
    "\n",
    "\n",
    "    for i in range(trials_vp-init_pts):\n",
    "        x0 = np.random.uniform(bounds.lb,\n",
    "                                   bounds.ub,\n",
    "                                   (restarts, dim))\n",
    "        for j in range(splitvar):\n",
    "            opt = Parallel(n_jobs = af_cores)(delayed(optimize_acqf(\n",
    "                                                                    acq_fun[str(j+1)],\n",
    "                                                                    bounds=bounds,\n",
    "                                                                    q=1, num_restarts=5,raw_samples=20,\n",
    "                                                                    inequality_constraints=ineq_constraint\n",
    "                                                                    )          \n",
    "                                                        )\n",
    "                                                  for x_0 in x0)\n",
    "            \n",
    "            X_test_new = np.array([res.x for res in opt], dtype = 'float')\n",
    "            acqf_val = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "            X_test_new[j] = X_test_new[np.argmin(acqf_val)]\n",
    "            X_test_new[-1, j*div:(j+1)*div] = X_test_new[np.argmin(acqf_val), j*div:(j+1)*div]\n",
    "\n",
    "        if f_cores == 1:\n",
    "                X_test_best[0] = X_test_new\n",
    "        else:\n",
    "                for j in range(f_cores-1):\n",
    "                    X_test_best[j] = X_test_new[j*splt:(j+1)*splt, :]\n",
    "                X_test_best[-1] = X_test[(j+1)*splt:, :]\n",
    "\n",
    "\n",
    "        start_f = time.time()\n",
    "        y_test_new = Parallel(n_jobs = f_cores)(delayed(self.distmod)(self.descale(x_s), *self.args)\n",
    "                                             for x_s in x_nxtbs)\n",
    "\n",
    "\n",
    "\n",
    "        time_fvp[i+init_pts] = time_fvp[i+init_pts-1]+(end_f-start_f)\n",
    "\n",
    "        X_train = torch.vstack((X_train, X_test_new))\n",
    "        y_train = torch.vstack((y_train, y_test_new))\n",
    "        y_test_new= torch.vstack((y_test_new[:]))\n",
    "        eps_next = y_test_new-y_test\n",
    "        eps = torch.vstack([eps, eps_next])\n",
    "        y_best = torch.vstack([y_best, np.min(y_test_new, axis = 0).reshape(-1,1).T])\n",
    "        \n",
    "        for j in range(splitvar):\n",
    "             if any(y_test_new[:,j] < min(y[:,j])):\n",
    "                lwr[j] = X_test_new[np.argmin(y_test_new[:, j])]\n",
    "                lwr[j, j*div:(j+1)*div] = bounds.lb[j]\n",
    "                upr[j] = X_test_new[np.argmin(y_test_new[:, j])]+1e-6\n",
    "                upr[j, j*div:(j+1)*div] = bounds.ub[j]  \n",
    "\n",
    "        for j in range(splitvar):\n",
    "            mll[str(i+1)] = fit_gpytorch_mll(mll)\n",
    "            acq_fun[str(i+1)] = ExpectedImprovement(model=gp[str(i+1)], \n",
    "                                                best_f=torch.min(y_train))\n",
    "            [str(j+1)].fit(x, eps[:, j])\n",
    "            bnds_var[str(j+1)] = Bounds(lwr[j], upr[j])          \n",
    "        end =time.time()\n",
    "        time_vp[i+init_pts] = end-start\n",
    "\n",
    "        print('ITERATION COUNT IS AT 'f'{init_pts+i+1};\\\n",
    "                  TOTAL ELAPSED TIME: 'f'{time_vp[i+init_pts]:.1f}')\n",
    "        \n",
    "    vpbo_optim = True\n",
    "    model_vp = gp\n",
    "    x_vp = descale(x)\n",
    "    y_vp = y\n",
    "    y_vpbst = y_best\n",
    "    \n",
    "\n",
    "\n",
    "        # 3.a (Optional) Find the closes test point\n",
    "        best_idx = -1\n",
    "        best_dist = np.inf\n",
    "        for idx, test_point in enumerate(X_test):\n",
    "            cur_dist = euclidean(X_new.squeeze(), test_point)\n",
    "            if cur_dist < best_dist:\n",
    "                best_dist = cur_dist\n",
    "                best_idx = idx\n",
    "\n",
    "        # 3.b Update the dataset by adding the new point\n",
    "        X_test_new = X_test[best_idx]\n",
    "        y_test_new = y_test[best_idx]\n",
    "\n",
    "        # Remove from test set\n",
    "        X_test = torch.cat((X_test[:best_idx], X_test[best_idx + 1:]))\n",
    "        y_test = torch.cat((y_test[:best_idx], y_test[best_idx + 1:]))\n",
    "\n",
    "        print(f\"Round: {i}\")\n",
    "        print(f\" - New experimental point: {X_new}\")\n",
    "        print(f\" - New acquisition value: {acqf_val}\")\n",
    "        print(f\" - Test point: {X_test_new}\")\n",
    "        print(f\" - Test evaluation: {y_test_new}\")\n",
    "        print(F\" - Dataset shape: {X_train.shape[0]}\")\n",
    "        print(\"\")\n",
    "\n",
    "        X_train = torch.vstack((X_train, X_test_new))\n",
    "        y_train = torch.vstack((y_train, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_vpbool(trials, split_num, liminit, fcores=1, afcores=1, refcores=1, x_init=x): \n",
    "    trialspltvar = trials\n",
    "    splitvar = split_num\n",
    "    div = int(dim/splitvar)\n",
    "    #sf = scaling_factor\n",
    "    \n",
    "    \n",
    "    x = liminit*np.ones((splitvar, dim))\n",
    "    lwr = x.copy()\n",
    "    upr = x.copy()+1e-6\n",
    "\n",
    "    for i in range(splitvar):\n",
    "        if x_init is None:\n",
    "            x[i, i*div:(i+1)*div] = np.random.uniform(  bounds.lb,\n",
    "                                                        bounds.ub,\n",
    "                                                        (1, div))\n",
    "        else:\n",
    "            x_init = x_init.reshape(1, dim)\n",
    "            x[i, i*div:(i+1)*div] = x_init[0, i*div:(i+1)*div]       \n",
    "        \n",
    "        lwr[i, i*div:(i+1)*div] = bounds.lb[i]\n",
    "        upr[i, i*div:(i+1)*div] = bounds.ub[i]\n",
    "    x = np.vstack([x, liminit])\n",
    "        \n",
    "    init_pts = int(len(x)/splitvar)\n",
    "    splt = int(x.shape[0]/fcores)\n",
    "    xbs = np.array(np.ones(fcores), dtype = tuple)\n",
    "\n",
    "    \n",
    "\n",
    "#results = Parallel(n_jobs=fcores)(delayed(run_trial)(params) for params in param_combinations[:trials])\n",
    "\n",
    "    if fcores == 1:\n",
    "        xbs[0] = x\n",
    "    else:\n",
    "        for i in range(fcores-1):\n",
    "            xbs[i] = x[i*splt:(i+1)*splt, :]\n",
    "        xbs[-1] = x[(i+1)*splt:, :]\n",
    "\n",
    "    start_f =time.time()\n",
    "    y= Parallel(n_jobs= fcores)(delayed(descale(x_s) for x_s in xbs))\n",
    "    \n",
    "    #y_ref=y_test\n",
    "    \n",
    "    y_test =Parallel(n_jobs = refcores)(delayed(descale(x_s) for x_s in xbs))\n",
    "    y_test = np.vstack(y_test[:])\n",
    "    end_f = time.time() \n",
    "    time_fvp[0] = end_f-start_f\n",
    "      \n",
    "    y = np.vstack(y[:])\n",
    "    eps = y-y_ref\n",
    "    y_best= np.min(y,azis = 0).reshape(-1, 1).T\n",
    "\n",
    "    mll = {}\n",
    "    gp = {}\n",
    "    acq_fun = {}\n",
    "\n",
    "# Simulate BO\n",
    "    for i in range(splitvar):\n",
    "        # 1. Create a new GP with the best setup \n",
    "        gp[str(i+1)], mll[str(i+1)] = get_gp(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            ConstantMean(),\n",
    "            ScaleKernel(RBFKernel()),\n",
    "            GaussianLikelihood()\n",
    "        )\n",
    "        \n",
    "        mll[str(i+1)] = fit_gpytorch_mll(mll)\n",
    "        bnds_var[str(i+1)] = Bounds(lwr[i], upr[i])\n",
    "        # 2. Define and optimize an acquisition function\n",
    "        acq_fun[str(i+1)] = ExpectedImprovement(model=gp[str(i+1)], \n",
    "                                                best_f=torch.min(y_train))\n",
    "\n",
    "    restarts = int(round(128/(splits+1), 0))\n",
    "    X_next = x.copy()\n",
    "    X_test_best = np.array(np.ones(f_cores), dtype = tuple)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i in range(init_pts):\n",
    "        self.time_vp[i] = (i+1)*(end-start)/init_pts\n",
    "        self.time_fvp[i] = (i+1)*(end_f-start_f)/init_pts\n",
    "    print('ITERATION COUNT IS AT 'f'{init_pts};\\\n",
    "        TOTAL ELAPSED TIME: 'f'{self.time_vp[init_pts-1]:.1f}')\n",
    "\n",
    "    end = time.time()\n",
    "    self.time_vp[0] = end-start\n",
    "\n",
    "\n",
    "    for i in range(trials_vp-init_pts):\n",
    "        x0 = np.random.uniform(bounds.lb,\n",
    "                                   bounds.ub,\n",
    "                                   (restarts, dim))\n",
    "        for j in range(splitvar):\n",
    "            opt = Parallel(n_jobs = af_cores)(delayed(optimize_acqf(\n",
    "                                                                    acq_fun[str(j+1)],\n",
    "                                                                    bounds=bounds,\n",
    "                                                                    q=1, num_restarts=5,raw_samples=20,\n",
    "                                                                    inequality_constraints=ineq_constraint\n",
    "                                                                    )          \n",
    "                                                        )\n",
    "                                                  for x_0 in x0)\n",
    "            \n",
    "            X_test_new = np.array([res.x for res in opt], dtype = 'float')\n",
    "            acqf_val = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
    "            X_test_new[j] = X_test_new[np.argmin(acqf_val)]\n",
    "            X_test_new[-1, j*div:(j+1)*div] = X_test_new[np.argmin(acqf_val), j*div:(j+1)*div]\n",
    "\n",
    "        if f_cores == 1:\n",
    "                X_test_best[0] = X_test_new\n",
    "        else:\n",
    "                for j in range(f_cores-1):\n",
    "                    X_test_best[j] = X_test_new[j*splt:(j+1)*splt, :]\n",
    "                X_test_best[-1] = X_test[(j+1)*splt:, :]\n",
    "\n",
    "\n",
    "        start_f = time.time()\n",
    "        y_test_new = Parallel(n_jobs = f_cores)(delayed(self.distmod)(self.descale(x_s), *self.args)\n",
    "                                             for x_s in x_nxtbs)\n",
    "\n",
    "\n",
    "\n",
    "        time_fvp[i+init_pts] = time_fvp[i+init_pts-1]+(end_f-start_f)\n",
    "\n",
    "        X_train = torch.vstack((X_train, X_test_new))\n",
    "        y_train = torch.vstack((y_train, y_test_new))\n",
    "        y_test_new= torch.vstack((y_test_new[:]))\n",
    "        eps_next = y_test_new-y_test\n",
    "        eps = torch.vstack([eps, eps_next])\n",
    "        y_best = torch.vstack([y_best, np.min(y_test_new, axis = 0).reshape(-1,1).T])\n",
    "        \n",
    "        for j in range(splitvar):\n",
    "             if any(y_test_new[:,j] < min(y[:,j])):\n",
    "                lwr[j] = X_test_new[np.argmin(y_test_new[:, j])]\n",
    "                lwr[j, j*div:(j+1)*div] = bounds.lb[j]\n",
    "                upr[j] = X_test_new[np.argmin(y_test_new[:, j])]+1e-6\n",
    "                upr[j, j*div:(j+1)*div] = bounds.ub[j]  \n",
    "\n",
    "        for j in range(splitvar):\n",
    "            mll[str(i+1)] = fit_gpytorch_mll(mll)\n",
    "            acq_fun[str(i+1)] = ExpectedImprovement(model=gp[str(i+1)], \n",
    "                                                best_f=torch.min(y_train))\n",
    "            [str(j+1)].fit(x, eps[:, j])\n",
    "            bnds_var[str(j+1)] = Bounds(lwr[j], upr[j])          \n",
    "        end =time.time()\n",
    "        time_vp[i+init_pts] = end-start\n",
    "\n",
    "        print('ITERATION COUNT IS AT 'f'{init_pts+i+1};\\\n",
    "                  TOTAL ELAPSED TIME: 'f'{time_vp[i+init_pts]:.1f}')\n",
    "        \n",
    "    vpbo_optim = True\n",
    "    model_vp = gp\n",
    "    x_vp = descale(x)\n",
    "    y_vp = y\n",
    "    y_vpbst = y_best\n",
    "    \n",
    "\n",
    "\n",
    "        # 3.a (Optional) Find the closes test point\n",
    "        best_idx = -1\n",
    "        best_dist = np.inf\n",
    "        for idx, test_point in enumerate(X_test):\n",
    "            cur_dist = euclidean(X_new.squeeze(), test_point)\n",
    "            if cur_dist < best_dist:\n",
    "                best_dist = cur_dist\n",
    "                best_idx = idx\n",
    "\n",
    "        # 3.b Update the dataset by adding the new point\n",
    "        X_test_new = X_test[best_idx]\n",
    "        y_test_new = y_test[best_idx]\n",
    "\n",
    "        # Remove from test set\n",
    "        X_test = torch.cat((X_test[:best_idx], X_test[best_idx + 1:]))\n",
    "        y_test = torch.cat((y_test[:best_idx], y_test[best_idx + 1:]))\n",
    "\n",
    "        print(f\"Round: {i}\")\n",
    "        print(f\" - New experimental point: {X_new}\")\n",
    "        print(f\" - New acquisition value: {acqf_val}\")\n",
    "        print(f\" - Test point: {X_test_new}\")\n",
    "        print(f\" - Test evaluation: {y_test_new}\")\n",
    "        print(F\" - Dataset shape: {X_train.shape[0]}\")\n",
    "        print(\"\")\n",
    "\n",
    "        X_train = torch.vstack((X_train, X_test_new))\n",
    "        y_train = torch.vstack((y_train, y_test_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
